# CBAM-AR-VQA

## The code of CBAM-AR-VQA model. This code is constructed based on the https://github.com/aioz-ai/MICCAI19-MedVQA and https://github.com/awenbocc/med-vqa. Kindly thank their sharing. All our source code will be released soon.

The datasets are available atÔºöhttps://pan.baidu.com/s/1FMLXBJhg5UR7-uC0C-xHmg with passpord "9urs" 

The pretrained weights are available atÔºöhttps://pan.baidu.com/s/1PisHX_2iIMgIZ4g4B-sSGw with passport "h9cv" 


## Experiments and Results
### Baselines
MEVF: Overcoming data limitation in medical visual question answering. InProceedings of International Conference on Medical Image Computing and Computer-Assisted Intervention.

MMQ: T. Do, B. Nguyen, E. Tjiputra, M. Tran, Q. Tran, and A. Nguyen. Multiple meta-model quantifying for medical visual question answering. In Proceedings of International Conference on Medical Image Computing and Computer-Assisted Intervention.

Q&T-CR: L. Zhan, B. Liu, L. Fan, J. Chen, and X. Wu. Medical visual questionanswering via conditional reasoning. In Proceedings of the 28th ACM International Conference on Multimedi.

MMBERT: Y. Khare, V. Bagal, M. Mathew, A. Devi, U. Priyakumar, and C. Jawahar. Mmbert: multimodal bert pretraining for improved medical vqa. In Proceedings of IEEE International Symposium on Biomedical Imaging.

CMSA: H. Gong, G. Chen, S. Liu, Y. Yu, and G. Li. Cross-modal self-attention with multi-task pre-training for medical visual question answering.
In Proceedings of the 2021 International Conference on Multimedia Retrieval.


### Datasets
VQA-RAD, VQA-Slake and PathVQA. 

### Results
![image](https://user-images.githubusercontent.com/35678614/198832513-3cd555ab-5b25-404c-8251-84601fbb8107.png)

Please note that the statistical significance is computed by the two-tailed ùëá-test method. The symbols ‚Üë‚Üë‚Üë, ‚Üë‚Üë and ‚Üë denote that our model outperforms the baseline models on significance level at ùëù-value < 0.01, < 0.05 and < 0. 1 respectively in the table. 

