# CBAM-AR-VQA

## The code of CBAM-AR-VQA model. This code is constructed based on the https://github.com/aioz-ai/MICCAI19-MedVQA and https://github.com/awenbocc/med-vqa. Kindly thank their sharing. All our source code will be released soon.

## Experiments and Results
### Baselines
MEVF: Overcoming data limitation in medical visual question answering. InProceedings of International Conference on Medical Image Computing and Computer-Assisted Intervention.

MMQ: T. Do, B. Nguyen, E. Tjiputra, M. Tran, Q. Tran, and A. Nguyen. Multiple meta-model quantifying for medical visual question answering. In Proceedings of International Conference on Medical Image Computing and Computer-Assisted Intervention.

Q&T-CR: L. Zhan, B. Liu, L. Fan, J. Chen, and X. Wu. Medical visual questionanswering via conditional reasoning. In Proceedings of the 28th ACM International Conference on Multimedi.

MMBERT: Y. Khare, V. Bagal, M. Mathew, A. Devi, U. Priyakumar, and C. Jawahar. Mmbert: multimodal bert pretraining for improved medical vqa. In Proceedings of IEEE International Symposium on Biomedical Imaging.

CMSA: H. Gong, G. Chen, S. Liu, Y. Yu, and G. Li. Cross-modal self-attention with multi-task pre-training for medical visual question answering.
In Proceedings of the 2021 International Conference on Multimedia Retrieval.


### Datasets
VQA-RAD, VQA-Slake and PathVQA. 
![image](https://user-images.githubusercontent.com/35678614/198832513-3cd555ab-5b25-404c-8251-84601fbb8107.png)

